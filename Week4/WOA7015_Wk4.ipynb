{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WOA7015_Wk4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsyhZGn3ZL75x1q7VeJOLC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiernee/Advanced_ML/blob/main/Week4/WOA7015_Wk4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5hkuTyx3VO"
      },
      "source": [
        "# Welcome to WOA7015 Advance Machine Learning Lab - Week 4\n",
        "This code is generated for the purpose of WOA7015 module.\n",
        "The code is available in github https://github.com/shiernee/Advanced_ML \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INxzH5Bqw5dq"
      },
      "source": [
        "## 1.0 Effect of weight and bias to sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah7KAg-fsGWY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "f = lambda x, w, b: 1/(1 + np.exp(-(w*x + b)))\n",
        "\n",
        "x = np.arange(-10, 10, 0.01).reshape([-1, 1])\n",
        "\n",
        "# effect of weight on sigmoid function\n",
        "filenames = []\n",
        "for i in np.arange(1, 5, 0.1):\n",
        "  w = np.ones([1, 1]) * i * 0.5\n",
        "  b = np.ones([1, 1]) * 0\n",
        "\n",
        "  plt.plot(x, f(x, w, b))\n",
        "  plt.title('w = %0.1f' % i)\n",
        "  plt.grid()\n",
        "  plt.savefig('w %0.1f.png' % i)\n",
        "  plt.close()\n",
        "  filenames.append('w %0.1f.png' % i)\n",
        "\n",
        "# Build GIF\n",
        "with imageio.get_writer('w_mygif.gif', mode='I') as writer:\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)\n",
        "\n",
        "\n",
        "# effect of bias on sigmoid function\n",
        "filenames = []\n",
        "for i in np.arange(1, 5, 0.1):\n",
        "  w = np.ones([1, 1]) \n",
        "  b = np.ones([1, 1])* i\n",
        "\n",
        "  plt.plot(x, f(x, w, b))\n",
        "  plt.title('b = %0.1f' % i)\n",
        "  plt.grid()\n",
        "  plt.savefig('b %0.1f.png' % i)\n",
        "  plt.close()\n",
        "  filenames.append('b %0.1f.png' % i)\n",
        "\n",
        "# Build GIF\n",
        "with imageio.get_writer('b_mygif.gif', mode='I') as writer:\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXAZiEqSEbDH"
      },
      "source": [
        "# 2.0 Logistic Regression\n",
        "\n",
        "In this section, we will learn how to create train a Logistic Regression Model using pytorch. We will use MNIST image, as shown below. \n",
        "\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/shiernee/Advanced_ML/main/Week4/MnistExamples.png\" width=\"512\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NXavLptEdGe"
      },
      "source": [
        "# 2.1 import library\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crKCbRxdFD86"
      },
      "source": [
        "#2.2 Set the Hyper-parameters \n",
        "input_size = 28 * 28    # 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZXRWEdzFt_Z"
      },
      "source": [
        "#2.3 Data loader\n",
        "# MNIST dataset (images and labels)\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader (input pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6ZE4roJF0HA"
      },
      "source": [
        "#2.4 Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXCxm64GH27K"
      },
      "source": [
        "#2.5 Cross Entropy Loss \n",
        "# nn.CrossEntropyLoss() computes softmax internally\n",
        "criterion = nn.CrossEntropyLoss()   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm2Q-PDcH34b"
      },
      "source": [
        "#2.6 Optimizer Stochastic Gradient Descent \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TO7Egs7F2o9"
      },
      "source": [
        "#2.7 Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        images = images.reshape(-1, input_size)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGO_mdMZF4iS"
      },
      "source": [
        "#2.8 Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, input_size)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBEBeGYsF5t-"
      },
      "source": [
        "#2.9 Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6nIkwr2YM9l"
      },
      "source": [
        "## Exercise 1 (10%): Create custom loss function\n",
        " 5% will be  given if step 1 - 4 are done correctly <br>\n",
        " 3% will be  given if step 5 are done correctly <br>\n",
        " 2% will be given if you get zero error. \n",
        "\n",
        "In this section, you will need to create our own Cross Entropy loss function and compare with Pytorch Cross Entropy loss. Follow the steps below:\n",
        "1. Import libraries - copy section 2.1\n",
        "2. Set hyperparameter - copy section 2.2\n",
        "3. Data loader - copy section 2.3\n",
        "4. Initialize Logistic Regression - copy section 2.4\n",
        "5. Create custom_CrossEntropyLoss class - copy the following code. Your job is to code the log_softmax equation in the log_softmax function. \n",
        "\n",
        "```\n",
        "#  Custom Loss - Cross Entropy Loss\n",
        "class custom_CrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(custom_CrossEntropyLoss, self).__init__()\n",
        " \n",
        "    def forward(self, inputs, targets, smooth=1):      \n",
        "        num_examples = targets.shape[0]\n",
        "        batch_size = inputs.shape[0]\n",
        "        softmax_outputs = self.log_softmax(inputs)\n",
        "        outputs = softmax_outputs[range(batch_size), targets]        \n",
        "        return -torch.sum(outputs)/num_examples\n",
        "\n",
        "    @staticmethod\n",
        "    def log_softmax(x):\n",
        "      return ### put the log_softmax function here ### \n",
        "```\n",
        "\n",
        "6. Initialize custom_CrossEntropyLoss loss as criterion - copy section 2.5. Replace *nn.CrossEntropyLoss* with *custom_CrossEntropyLoss2*\n",
        "7. Train the model - copy section 2.6. Uncomment the *print code*. Comment away the code after *# Backward and optimize*. \n",
        "\n",
        "```\n",
        "        ### uncomment the following code ###\n",
        "        # print('torch loss:', criterion(outputs, labels))\n",
        "        # print('custom loss:', custom_criterion(outputs, labels))\n",
        "\n",
        "        ### comment the following code ###\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "```\n",
        "\n",
        "8. Compare the loss computed from torch and our custom loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azur5ciavP2"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7IdSoak0p2"
      },
      "source": [
        "# Submission Instructions\n",
        "Once you are finished, follow these steps:\n",
        "\n",
        "Restart the kernel and re-run this notebook from beginning to end by going to Kernel > Restart Kernel and Run All Cells.\n",
        "If this process stops halfway through, that means there was an error. Correct the error and repeat Step 1 until the notebook runs from beginning to end.\n",
        "Double check that there is a number next to each code cell and that these numbers are in order.\n",
        "Then, submit your lab as follows:\n",
        "\n",
        "Go to File > Export Notebook As > PDF.\n",
        "Double check that the entire notebook, from beginning to end, is in this PDF file. (If the notebook is cut off, try first exporting the notebook to HTML and printing to PDF.) Make sure Solution for Exercise 4 & 5 are in for marks. \n",
        "Upload the PDF to Spectrum. "
      ]
    }
  ]
}