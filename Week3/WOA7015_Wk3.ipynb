{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WOA7015_Wk3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOobom9+tMdacpbqj1UkLow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiernee/Advanced_ML/blob/main/Week3/WOA7015_Wk3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzY5ZsYGoDdu"
      },
      "source": [
        "# Welcome to WOA7015 Advance Machine Learning Lab - Week 3\n",
        "This code is generated for the purpose of WOA7015 module.\n",
        "The code is available in github https://github.com/shiernee/Advanced_ML \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVerYxHW-ZrQ"
      },
      "source": [
        "# The effect of imbalanced data on AUROC \n",
        "The following code evaluates the effect of imbalanced data on the AUROC of TPR-FPR curve. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c87yzg0goBrP"
      },
      "source": [
        "# roc curve and auc on an imbalanced dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.under_sampling import RandomUnderSampler\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcpJntDPEq2J",
        "outputId": "6e6cbb8c-8ee1-43a6-91bc-64bdfad9412f"
      },
      "source": [
        "# generate 2 class dataset \n",
        "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1000)\n",
        "\n",
        "print(X)\n",
        "print('-----------')\n",
        "print(y)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.32584935  0.21897754  0.62061895 ...  2.84071377 -0.02582733\n",
            "  -0.40885762]\n",
            " [-1.12624124 -0.86026727 -0.89264356 ... -0.92962064  0.59483549\n",
            "   1.24052468]\n",
            " [-0.48993428 -0.7453348  -1.43801838 ... -1.67525801 -0.09994425\n",
            "  -0.46569289]\n",
            " ...\n",
            " [ 0.47406074 -1.9209351   0.41681779 ...  1.04574815  1.092832\n",
            "  -0.01541749]\n",
            " [-0.62731673 -0.94336697 -1.50694171 ... -0.85092941  0.99046917\n",
            "   2.19583454]\n",
            " [ 0.88990126  0.81857103 -2.12551556 ...  1.00271323 -0.88101446\n",
            "  -0.81149645]]\n",
            "-----------\n",
            "[1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLo12OKVE__J",
        "outputId": "a6dbfbe9-fecd-4d53-b281-c7193c79caa2"
      },
      "source": [
        "# split into train/test sets\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=1000)\n",
        "\n",
        "print('trainy - class0: ', len(trainy)-trainy.sum())\n",
        "print('trainy - class1: ', trainy.sum())\n",
        "print('----------------------')\n",
        "print('testy - class0: ', len(testy)-testy.sum())\n",
        "print('testy - class1: ', testy.sum())\n",
        "print('============================')\n",
        "\n",
        "# make testing dataset balance\n",
        "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "testX, testy = undersample.fit_resample(testX, testy)\n",
        "\n",
        "print('Balanced Testing date')\n",
        "print('testy - class0: ', len(testy)-testy.sum())\n",
        "print('testy - class1: ', testy.sum())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainy - class0:  253\n",
            "trainy - class1:  247\n",
            "----------------------\n",
            "testy - class0:  249\n",
            "testy - class1:  251\n",
            "============================\n",
            "Balanced Testing date\n",
            "testy - class0:  249\n",
            "testy - class1:  249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3QmySaLE7Nm"
      },
      "source": [
        "# fit a model with training data\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(trainX, trainy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOu_783uGpZd"
      },
      "source": [
        "# repeat with different skewness \n",
        "roc_list = []\n",
        "lr_acc = []\n",
        "k=1\n",
        "for i in range(0, 10):\n",
        "  pos_ind = np.where(testy==1)[0]\n",
        "  n = int(i/10 * len(pos_ind))\n",
        "  tmp_testX, tmp_testy = np.copy(testX), np.copy(testy)\n",
        "  tmp_testX = np.delete(tmp_testX, pos_ind[:n], axis=0)\n",
        "  tmp_testy = np.delete(tmp_testy, pos_ind[:n], axis=0)\n",
        "  print('nth %d:positive: %d negative: %d' \n",
        "        % (i, tmp_testy.sum(), tmp_testy.shape[0] - tmp_testy.sum()))\n",
        "  print('---------------------------------------------')\n",
        "  \n",
        "  # predict probabilities\n",
        "  lr_probs = model.predict_proba(tmp_testX)\n",
        "  # keep probabilities for the positive outcome only\n",
        "  lr_probs = lr_probs[:, 1]\n",
        "  # calculate scores\n",
        "  lr_auc = roc_auc_score(tmp_testy, lr_probs)\n",
        "\n",
        "  # summarize scores\n",
        "  # print('iteration %d: Logistic: ROC AUC=%.3f' % (k, lr_auc))\n",
        "  k += 1\n",
        "  # calculate roc curves\n",
        "  lr_fpr, lr_tpr, _ = roc_curve(tmp_testy, lr_probs)\n",
        "  roc_list.append(lr_auc)\n",
        "\n",
        "plt.plot(np.arange(0, len(roc_list)), roc_list)\n",
        "plt.xlabel('skewness ratio')\n",
        "plt.ylabel('AUROC')\n",
        "plt.title('decreasing positive sample')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njFP2GHpC1VE"
      },
      "source": [
        "# Exercise 1 (2%):\n",
        "Does the AUROC (TPR vs FPR) affected by imbalanced class?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOIpcKliC56h"
      },
      "source": [
        "# Your answer here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4Cxp0q75PM"
      },
      "source": [
        "# The effect of imbalanced data on AUROC of PR curve and F1 score\n",
        "The following code evaluates the effect of imbalanced data on the AUROC of Precision-Recall and F1 value. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYxjJuD_8ewJ"
      },
      "source": [
        "# roc curve and auc on an imbalanced dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr_uY_mPHLTF"
      },
      "source": [
        "# generate 2 class dataset \n",
        "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1000)\n",
        "\n",
        "print(X)\n",
        "print('-----------')\n",
        "print(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbmv6pFhHWyQ"
      },
      "source": [
        "# split into train/test sets\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=1000)\n",
        "\n",
        "print('trainy - class0: ', len(trainy)-trainy.sum())\n",
        "print('trainy - class1: ', trainy.sum())\n",
        "print('----------------------')\n",
        "print('testy - class0: ', len(testy)-testy.sum())\n",
        "print('testy - class1: ', testy.sum())\n",
        "print('============================')\n",
        "\n",
        "# make testing dataset balance\n",
        "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "testX, testy = undersample.fit_resample(testX, testy)\n",
        "\n",
        "print('Balanced Testing date')\n",
        "print('testy - class0: ', len(testy)-testy.sum())\n",
        "print('testy - class1: ', testy.sum())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1ST-YEcHh5I"
      },
      "source": [
        "# fit a model\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(trainX, trainy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scPocodDHRp-"
      },
      "source": [
        "# repeat with different skewness \n",
        "roc_list = []\n",
        "f1_list = []\n",
        "\n",
        "k=1\n",
        "for i in range(0, 10):\n",
        "  pos_ind = np.where(testy==1)[0]\n",
        "  n = int(i/10 * len(pos_ind))\n",
        "  tmp_testX, tmp_testy = np.copy(testX), np.copy(testy)\n",
        "  tmp_testX = np.delete(tmp_testX, pos_ind[:n], axis=0)\n",
        "  tmp_testy = np.delete(tmp_testy, pos_ind[:n], axis=0)\n",
        "  print('nth %d:positive: %d negative: %d' \n",
        "        % (i, tmp_testy.sum(), tmp_testy.shape[0] - tmp_testy.sum()))\n",
        "  print('---------------------------------------------')\n",
        "  \n",
        "\n",
        "  # predict probabilities\n",
        "  lr_probs = model.predict_proba(tmp_testX)\n",
        "  # keep probabilities for the positive outcome only\n",
        "  lr_probs = lr_probs[:, 1]\n",
        "  # predict class values\n",
        "  yhat = model.predict(tmp_testX)\n",
        "  # calculate precision and recall for each threshold\n",
        "  lr_precision, lr_recall, _ = precision_recall_curve(tmp_testy, lr_probs)\n",
        "  # calculate scores\n",
        "  lr_f1, lr_auc = f1_score(tmp_testy, yhat), auc(lr_recall, lr_precision)\n",
        "  # summarize scores\n",
        "  # print('iteration%d Logistic: f1=%.3f auc=%.3f' % (k, lr_f1, lr_auc))\n",
        "  k += 1\n",
        "  roc_list.append(lr_auc)\n",
        "  f1_list.append(lr_f1)\n",
        "\n",
        "plt.plot(np.arange(0, len(roc_list)), roc_list)\n",
        "plt.xlabel('skewness ratio')\n",
        "plt.ylabel('AUC of PR curve')\n",
        "plt.title('decreasing positive sample')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, len(roc_list)), f1_list)\n",
        "plt.xlabel('skewness ratio')\n",
        "plt.ylabel('F1')\n",
        "plt.title('decreasing positive sample')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DP3LOZY7v6M"
      },
      "source": [
        "# Exercise 2 (4%):\n",
        "Does the AUROC (Precision vs Recall), F1 score affected by imbalanced class?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPXGu2E00-c5"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h1qUoy4DSc2"
      },
      "source": [
        "# ***Let's go back to power point - slide 13***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3GJxwgzEjP"
      },
      "source": [
        "# Convex function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v0il6m3zOQb"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "x = np.arange(-2, 2, 0.01)\n",
        "\n",
        "# choose one function to try\n",
        "f = lambda x: 0.5 * x ** 2 # Convex\n",
        "# f = lambda x: np.cos(np.pi * x)  # Nonconvex\n",
        "# f = lambda x: -0.5 * x ** 4  # Nonconvex\n",
        "\n",
        "filenames=[]\n",
        "for lamda in np.arange(0, 1, 0.02):\n",
        "  # LHS\n",
        "  tmp_x = lamda*x[0] + (1-lamda)*x[-1]\n",
        "\n",
        "  # RHS\n",
        "  x_line, y_line = np.array([x[0], x[-1]]), np.array([lamda*f(x[0]), (1-lamda)*f(x[-1])])\n",
        "\n",
        "  # compute LHS and RHS\n",
        "  LHS = f(tmp_x)\n",
        "  RHS = lamda*f(x[0]) + (1-lamda)*f(x[-1])\n",
        "  if LHS > RHS:\n",
        "    print('At lamda %0.3f, it is concave' % lamda)\n",
        "    print('lhs %.5f rhs %.5f' % (LHS, RHS))\n",
        "\n",
        "  plt.figure()\n",
        "  # original graph\n",
        "  plt.plot(x, f(x), label='f(x)')\n",
        "  # plot RHS\n",
        "  plt.plot(x_line, y_line, label='%0.3f' % lamda)\n",
        "  # plot LHS\n",
        "  plt.scatter(tmp_x, f(tmp_x))\n",
        "  #title, legennd\n",
        "  plt.title('lhs %.3f rhs %.3f' % (LHS, RHS))\n",
        "  plt.legend()\n",
        "  # plt.savefig('lamda %0.3f.png' % lamda)\n",
        "  # plt.close()\n",
        "  filenames.append('lamda %0.3f.png' % lamda)\n",
        "\n",
        "# Build GIF\n",
        "with imageio.get_writer('mygif.gif', mode='I') as writer:\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Wuu9o0pGmg"
      },
      "source": [
        "# Understand how learning rate affects your deep learning\n",
        "\n",
        "We will train a neural network for a pretty simple task, i.e. calculating the exclusive-or (XOR) of two input. \n",
        "\n",
        "<br> \n",
        "<img src=\"https://raw.githubusercontent.com/shiernee/Advanced_ML/main/Week3/XOR.jpg\" width=\"512\"/>\n",
        "\n",
        "<br><br> \n",
        "For this experiment, try various values for the learning rate. Answer the following questions:\n",
        "1. For what range of values does the network learn and reach 100% accuracy? \n",
        "2. For what values does the network training diverge and become unstable? \n",
        "3. For what values does it learn too slowly to each optimal accuracy in 100 epochs?\n",
        "\n",
        "Give your answer at the second last cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RiENlDbpGS3"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        " \n",
        "# make data for an XOR model \n",
        "def make_data():\n",
        "    x1 = random.randint(0, 1)\n",
        "    x2 = random.randint(0, 1)\n",
        "    yy = 0 if (x1 == x2) else 1\n",
        " \n",
        "    # centered at zero\n",
        "    x1 = 2. * (x1 - 0.5)\n",
        "    x2 = 2. * (x2 - 0.5)\n",
        "    yy = 2. * (yy - 0.5)\n",
        " \n",
        "    # add noise\n",
        "    x1 += 0.1 * random.random()\n",
        "    x2 += 0.1 * random.random()\n",
        "    yy += 0.1 * random.random()\n",
        " \n",
        "    return [x1, x2, ], yy\n",
        " \n",
        "batch_size = 10\n",
        "def make_batch():\n",
        "    data = [make_data() for ii in range(batch_size)]\n",
        "    labels = [label for xx, label in data]\n",
        "    data = [xx for xx, label in data]\n",
        "    return np.array(data, dtype='float32'), np.array(labels, dtype='float32')\n",
        " \n",
        "print(make_batch())\n",
        "print(make_batch())\n",
        "print(make_batch())\n",
        " \n",
        "train_data = [make_batch() for ii in range(500)]\n",
        "test_data = [make_batch() for ii in range(50)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huxK2x7WpUGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0437e37c-dd14-4f4c-df10-fd7dc2443704"
      },
      "source": [
        "## Define our neural network \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        " \n",
        "torch.manual_seed(42)\n",
        " \n",
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        " \n",
        "        self.dense1 = nn.Linear(2, 2)\n",
        "        self.dense2 = nn.Linear(2, 1)\n",
        " \n",
        "        print(self.dense1.weight)\n",
        "        print(self.dense1.bias)\n",
        "        print(self.dense2.weight)\n",
        "        print(self.dense2.bias)\n",
        " \n",
        "        # self.dense1.weight.data.uniform_(-1.0, 1.0)\n",
        "        # self.dense1.bias.data.uniform_(-1.0, 1.0)\n",
        "        # self.dense2.weight.data.uniform_(-1.0, 1.0)\n",
        "        # self.dense2.bias.data.uniform_(-1.0, 1.0)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.dense1(x))\n",
        "        x = self.dense2(x)\n",
        "        return torch.squeeze(x)\n",
        " \n",
        "model = NN()\n",
        " \n",
        "## optimizer = stochastic gradient descent\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0005)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.5406,  0.5869],\n",
            "        [-0.1657,  0.6496]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1549,  0.1427], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.3443,  0.4153]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.6233], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Furjt7ZppdxA"
      },
      "source": [
        "## train and test functions\n",
        " \n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_data):\n",
        "        data, target = Variable(torch.from_numpy(data)), Variable(torch.from_numpy(target))\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.mse_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} {}\\tLoss: {:.4f}'.format(epoch, batch_idx * len(data), loss.item()))\n",
        " \n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_data:\n",
        "        data, target = Variable(torch.from_numpy(data), volatile=True), Variable(torch.from_numpy(target))\n",
        "        output = model(data)\n",
        "        test_loss += F.mse_loss(output, target)\n",
        "        correct += (np.around(output.data.numpy()) == np.around(target.data.numpy())).sum()\n",
        " \n",
        "    test_loss /= len(test_data)\n",
        "    test_loss = test_loss.item()\n",
        " \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, batch_size * len(test_data), 100. * correct / (batch_size * len(test_data))) )\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ4c_vUzphKy"
      },
      "source": [
        "## run experiment \n",
        " \n",
        "nepochs = 100\n",
        "for epoch in range(1, nepochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM3QoS2buZe7"
      },
      "source": [
        "## Exercise 3 (6%) \n",
        "For this experiment, try various values for the learning rate. Answer the following questions:\n",
        "1. For what range of values does the network learn and reach 100% accuracy? \n",
        "2. For what values does the network training diverge and become unstable? \n",
        "3. For what values does it learn too slowly to each optimal accuracy in 100 epochs?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5tn9G1ugD1"
      },
      "source": [
        "### Your answer here\n",
        "Q1: \n",
        "<br> Q2:\n",
        "<br> Q3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1d47bLhDMlw"
      },
      "source": [
        "# Submission Instructions\n",
        "Once you are finished, follow these steps:\n",
        "\n",
        "Restart the kernel and re-run this notebook from beginning to end by going to Kernel > Restart Kernel and Run All Cells.\n",
        "If this process stops halfway through, that means there was an error. Correct the error and repeat Step 1 until the notebook runs from beginning to end.\n",
        "Double check that there is a number next to each code cell and that these numbers are in order.\n",
        "Then, submit your lab as follows:\n",
        "\n",
        "Go to File > Print > Save as PDF.\n",
        "Double check that the entire notebook, from beginning to end, is in this PDF file. Make sure Solution for Exercise 5 are in for marks. \n",
        "Upload the PDF to Spectrum. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FBd4KLZwyVB"
      },
      "source": [
        "# Acknowledgement\n",
        "\n",
        "Some of the works are inspired from \n",
        "1. Effect of learning rate on AI model = https://www.commonlounge.com/discussion/5076b2cfb2364594ba608fca3ac606bb"
      ]
    }
  ]
}